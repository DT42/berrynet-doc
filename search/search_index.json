{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep Learning Gateway on Raspberry Pi And Other Edge Devices Supporting BerryNet Become a backer or sponsor on Open Collective . One-time donation via PayPal or crypto-currencies . Introduction This project turns edge devices such as Raspberry Pi into an intelligent gateway with deep learning running on it. No internet connection is required, everything is done locally on the edge device itself. Further, multiple edge devices can create a distributed AIoT network. At DT42, we believe that bringing deep learning to edge devices is the trend towards the future. It not only saves costs of data transmission and storage but also makes devices able to respond according to the events shown in the images or videos without connecting to the cloud. Figure 1: BerryNet architecture Figure 1 shows the software architecture of the project, we use Node.js/Python, MQTT and an AI engine to analyze images or video frames with deep learning. So far, there are two default types of AI engines, the classification engine (with Inception v3 [1] model) and the object detection engine (with TinyYOLO [2] model or MobileNet SSD [3] model). Figure 2 shows the differences between classification and object detection. Figure 2: Classification vs detection One of the application of this intelligent gateway is to use the camera to monitor the place you care about. For example, Figure 3 shows the analyzed results from the camera hosted in the DT42 office. The frames were captured by the IP camera and they were submitted into the AI engine. The output from the AI engine will be shown in the dashboard. We are working on the Email and IM notification so you can get a notification when there is a dog coming into the meeting area with the next release. Figure 3: Object detection result example To bring easy and flexible edge AI experience to user, we keep expending support of the AI engines and the reference HWs. Figure 4: Reference hardwares Installation You can install BerryNet by using pre-built image or from source. Please refer to the installation guide for the details. We are pushing BerryNet into Debian repository, so you will be able to install by only typing one command in the future. Here is the quick steps to install from source: $ git clone https://github.com/DT42/BerryNet.git $ cd BerryNet $ ./configure Start and Stop BerryNet BerryNet performs an AIoT application by connecting independent components together. Component types include but not limited to AI engine, I/O processor, data processor (algorithm), or data collector. We recommend to manage BerryNet componetns by supervisor , but you can also run BerryNet components manually. You can manage BerryNet via supervisorctl : ``` # Check status of BerryNet components $ sudo supervisorctl status all # Stop Camera client $ sudo supervisorctl stop camera # Restart all components $ sudo supervisorctl restart all # Show last stderr logs of camera client $ sudo supervisorctl tail camera stderr ``` For more possibilities of supervisorctl, please refer to the official tutorial . The default application has three components: Camera client to provide input images Object detection engine to find type and position of the detected objects in an image Dashboard to display the detection results You will learn how to configure or change the components in the Configuration section. Dashboard: Freeboard Open Freeboard on RPi (with touch screen) Freeboard is a web-based dashboard. Here are the steps to show the detection result iamge and text on Freeboard: 1: Enter http://127.0.0.1:8080 in browser's URL bar, and press enter 2: Download the Freeboard configuration for default application, dashboard-tflitedetector.json 2: Click LOAD FREEBOARD , and select the newly downloaded dashboard-tflitedetector.json 3: Wait for seconds, you should see the inference result image and text on Freeboard Open Freeboard on another computer Assuming that you have two devices: Device A with IP 192.168.1.42 , BerryNet default application runs on it Device B with IP 192.168.1.43 , you want to open Freeboard and see the detection result on it Here are the steps: 1: Enter http://192.168.1.42:8080 in browser's URL bar, and press enter 2: Download the Freeboard configuration for default application, dashboard-tflitedetector.json 3: Replace all the localhost to 192.168.1.42 in dashboard-tflitedetector.json 2: Click LOAD FREEBOARD , and select the newly downloaded dashboard-tflitedetector.json 3: Wait for seconds, you should see the inference result image and text on Freeboard For more details about dashboard configuration (e.g. how to add widgets), please refer to Freeboard project . Enable Data Collector You might want to store the snapshot and inference results for data analysis. To run BerryNet data collector manually, you can run the command below: $ bn_data_collector --topic-config <topic-config-filepath> --data-dirpath <result-dirpath> The topic config indicates what MQTT topic the data collector will listen, and what handler will be triggered. Here is a topic config exmaple: { \"berrynet/engine/tflitedetector/result\": \"self.update\" } The inference result image and text will be saved into the indicated result directory. Configuration The default supervisor config is at /etc/supervisor/conf.d/berrynet-tflite.conf . To write your own supervisor config, you can refer to here for more example supervisor configs of BerryNet Camera Client BerryNet camera client can run in two modes: stream or file. In stream mode, local camera (e.g. USB camera and RPi camera) and IP camera can be supported, and input frame rate (FPS) can be changed on demand (default is 1). In file mode, user can indicate filepath as input source. To run camera client in stream mode: $ bn_camera --fps 5 To run camera client in file mode: $ bn_camera --mode file --filepath <image-filepath> Use Your Data To Train The original instruction of retraining YOLOv2 model see github repository of darknet In the current of BerryNet, TinyYolo is used instead of YOLOv2. The major differences are: Create file yolo-obj.cfg with the same content as in tiny-yolo.cfg Download pre-trained weights of darknet reference model, darknet.weights.12 , for the convolutional layers (6.1MB) https://drive.google.com/drive/folders/0B-oZJEwmkAObMzAtc2QzZDhyVGM?usp=sharing The rest parts are the same as retraining YOLO. If you use LabelMe to annotate data, utils/xmlTotxt.py can help convert the xml format to the text format that darknet uses. Discussion Please refer to the Slack , Telegram Group or Google Group for questions, suggestions, or any idea discussion.","title":"Home"},{"location":"#introduction","text":"This project turns edge devices such as Raspberry Pi into an intelligent gateway with deep learning running on it. No internet connection is required, everything is done locally on the edge device itself. Further, multiple edge devices can create a distributed AIoT network. At DT42, we believe that bringing deep learning to edge devices is the trend towards the future. It not only saves costs of data transmission and storage but also makes devices able to respond according to the events shown in the images or videos without connecting to the cloud. Figure 1: BerryNet architecture Figure 1 shows the software architecture of the project, we use Node.js/Python, MQTT and an AI engine to analyze images or video frames with deep learning. So far, there are two default types of AI engines, the classification engine (with Inception v3 [1] model) and the object detection engine (with TinyYOLO [2] model or MobileNet SSD [3] model). Figure 2 shows the differences between classification and object detection. Figure 2: Classification vs detection One of the application of this intelligent gateway is to use the camera to monitor the place you care about. For example, Figure 3 shows the analyzed results from the camera hosted in the DT42 office. The frames were captured by the IP camera and they were submitted into the AI engine. The output from the AI engine will be shown in the dashboard. We are working on the Email and IM notification so you can get a notification when there is a dog coming into the meeting area with the next release. Figure 3: Object detection result example To bring easy and flexible edge AI experience to user, we keep expending support of the AI engines and the reference HWs. Figure 4: Reference hardwares","title":"Introduction"},{"location":"#installation","text":"You can install BerryNet by using pre-built image or from source. Please refer to the installation guide for the details. We are pushing BerryNet into Debian repository, so you will be able to install by only typing one command in the future. Here is the quick steps to install from source: $ git clone https://github.com/DT42/BerryNet.git $ cd BerryNet $ ./configure","title":"Installation"},{"location":"#start-and-stop-berrynet","text":"BerryNet performs an AIoT application by connecting independent components together. Component types include but not limited to AI engine, I/O processor, data processor (algorithm), or data collector. We recommend to manage BerryNet componetns by supervisor , but you can also run BerryNet components manually. You can manage BerryNet via supervisorctl : ``` # Check status of BerryNet components $ sudo supervisorctl status all # Stop Camera client $ sudo supervisorctl stop camera # Restart all components $ sudo supervisorctl restart all # Show last stderr logs of camera client $ sudo supervisorctl tail camera stderr ``` For more possibilities of supervisorctl, please refer to the official tutorial . The default application has three components: Camera client to provide input images Object detection engine to find type and position of the detected objects in an image Dashboard to display the detection results You will learn how to configure or change the components in the Configuration section.","title":"Start and Stop BerryNet"},{"location":"#dashboard-freeboard","text":"","title":"Dashboard: Freeboard"},{"location":"#open-freeboard-on-rpi-with-touch-screen","text":"Freeboard is a web-based dashboard. Here are the steps to show the detection result iamge and text on Freeboard: 1: Enter http://127.0.0.1:8080 in browser's URL bar, and press enter 2: Download the Freeboard configuration for default application, dashboard-tflitedetector.json 2: Click LOAD FREEBOARD , and select the newly downloaded dashboard-tflitedetector.json 3: Wait for seconds, you should see the inference result image and text on Freeboard","title":"Open Freeboard on RPi (with touch screen)"},{"location":"#open-freeboard-on-another-computer","text":"Assuming that you have two devices: Device A with IP 192.168.1.42 , BerryNet default application runs on it Device B with IP 192.168.1.43 , you want to open Freeboard and see the detection result on it Here are the steps: 1: Enter http://192.168.1.42:8080 in browser's URL bar, and press enter 2: Download the Freeboard configuration for default application, dashboard-tflitedetector.json 3: Replace all the localhost to 192.168.1.42 in dashboard-tflitedetector.json 2: Click LOAD FREEBOARD , and select the newly downloaded dashboard-tflitedetector.json 3: Wait for seconds, you should see the inference result image and text on Freeboard For more details about dashboard configuration (e.g. how to add widgets), please refer to Freeboard project .","title":"Open Freeboard on another computer"},{"location":"#enable-data-collector","text":"You might want to store the snapshot and inference results for data analysis. To run BerryNet data collector manually, you can run the command below: $ bn_data_collector --topic-config <topic-config-filepath> --data-dirpath <result-dirpath> The topic config indicates what MQTT topic the data collector will listen, and what handler will be triggered. Here is a topic config exmaple: { \"berrynet/engine/tflitedetector/result\": \"self.update\" } The inference result image and text will be saved into the indicated result directory.","title":"Enable Data Collector"},{"location":"#configuration","text":"The default supervisor config is at /etc/supervisor/conf.d/berrynet-tflite.conf . To write your own supervisor config, you can refer to here for more example supervisor configs of BerryNet","title":"Configuration"},{"location":"#camera-client","text":"BerryNet camera client can run in two modes: stream or file. In stream mode, local camera (e.g. USB camera and RPi camera) and IP camera can be supported, and input frame rate (FPS) can be changed on demand (default is 1). In file mode, user can indicate filepath as input source. To run camera client in stream mode: $ bn_camera --fps 5 To run camera client in file mode: $ bn_camera --mode file --filepath <image-filepath>","title":"Camera Client"},{"location":"#use-your-data-to-train","text":"The original instruction of retraining YOLOv2 model see github repository of darknet In the current of BerryNet, TinyYolo is used instead of YOLOv2. The major differences are: Create file yolo-obj.cfg with the same content as in tiny-yolo.cfg Download pre-trained weights of darknet reference model, darknet.weights.12 , for the convolutional layers (6.1MB) https://drive.google.com/drive/folders/0B-oZJEwmkAObMzAtc2QzZDhyVGM?usp=sharing The rest parts are the same as retraining YOLO. If you use LabelMe to annotate data, utils/xmlTotxt.py can help convert the xml format to the text format that darknet uses.","title":"Use Your Data To Train"},{"location":"#discussion","text":"Please refer to the Slack , Telegram Group or Google Group for questions, suggestions, or any idea discussion.","title":"Discussion"},{"location":"architecture/","text":"NOTE: This page is not completed yet Overview Work in Progress Communication MQTT topic format is based on the concept: berrynet/<component-type>/<component-name>/<message-type>[/message-additional-info...] MQTT topics using in BerryNet AI services berrynet/engine/darknet/result berrynet/engine/ovclassifier/result berrynet/engine/ovdetector/result berrynet/engine/pipeline/result berrynet/engine/tensorflow/result berrynet/engine/tfliteclassifier/result berrynet/engine/tflitedetector/result Clients berrynet/data/rgbimage berrynet/dashboard/snapshot berrynet/dashboard/inferenceResult Generic Inference Result Format Generic inference result format makes AI inference services to follow the same rule. Classification { \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"classification\", \"label\": STRING, \"confidence\": FLOAT32 }, ... ] } Detection { \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"detection\", \"label\": STRING, \"confidence\": FLOAT32, \"top\": UINT32, \"bottom\": UINT32, \"left\": UINT32, \"right\", UINT32, }, ... ] } Field Description Field Description Type timestamp Datetime string in ISO format. STRING bytes Raw image or image with inference results in base64 format. STRING type Inference type. Valid values: {classification, detection} STRING label Inference result label. STRING confidence Inference result confidence. Valid value: 0.00 <= confidence <= 1.00 FLOAT32 left x of the top-left point. UINT32 top y of the top-left point. UINT32 right x of the bottom-right point. UINT32 bottom y of the bottom-right point. UINT32","title":"Architecture"},{"location":"architecture/#overview","text":"Work in Progress","title":"Overview"},{"location":"architecture/#communication","text":"MQTT topic format is based on the concept: berrynet/<component-type>/<component-name>/<message-type>[/message-additional-info...] MQTT topics using in BerryNet AI services berrynet/engine/darknet/result berrynet/engine/ovclassifier/result berrynet/engine/ovdetector/result berrynet/engine/pipeline/result berrynet/engine/tensorflow/result berrynet/engine/tfliteclassifier/result berrynet/engine/tflitedetector/result Clients berrynet/data/rgbimage berrynet/dashboard/snapshot berrynet/dashboard/inferenceResult","title":"Communication"},{"location":"architecture/#generic-inference-result-format","text":"Generic inference result format makes AI inference services to follow the same rule.","title":"Generic Inference Result Format"},{"location":"architecture/#classification","text":"{ \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"classification\", \"label\": STRING, \"confidence\": FLOAT32 }, ... ] }","title":"Classification"},{"location":"architecture/#detection","text":"{ \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"detection\", \"label\": STRING, \"confidence\": FLOAT32, \"top\": UINT32, \"bottom\": UINT32, \"left\": UINT32, \"right\", UINT32, }, ... ] }","title":"Detection"},{"location":"architecture/#field-description","text":"Field Description Type timestamp Datetime string in ISO format. STRING bytes Raw image or image with inference results in base64 format. STRING type Inference type. Valid values: {classification, detection} STRING label Inference result label. STRING confidence Inference result confidence. Valid value: 0.00 <= confidence <= 1.00 FLOAT32 left x of the top-left point. UINT32 top y of the top-left point. UINT32 right x of the bottom-right point. UINT32 bottom y of the bottom-right point. UINT32","title":"Field Description"},{"location":"mkdocs/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Head 3 Content in Head 3.","title":"Welcome to MkDocs"},{"location":"mkdocs/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"mkdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"mkdocs/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"mkdocs/#head-3","text":"Content in Head 3.","title":"Head 3"},{"location":"clients/camera/","text":"BerryNet camera client ( bn_camera ) supports three types of input sources Physically connected camera, e.g. USB camera and Pi camera. Internet camera (IP camera) providing RTSP or MJPEG video stream. Single image for development or debugging. We will introduce how to use these different types of input sources below. USB Camera By default, camera client will use physically connected camera as input source: $ bn_camera You can use FPS parameter to control its image decoding rate: $ bn_camera --fps 5 RPi Camera The usage of RPi camera is the same as USB camera above. How to verify PiCamera HW IP Camera Before setting RTSP/MJPEG video stream as input source, I recommend to verify that the video stream can be played by a media player (e.g. VLC media player or Totem movie player). If input is RTSP video stream $ bn_camera --stream-src rtsp://<stream-rtsp-url> [--fps N] If input is MJPEG video stream $ bn_camera --stream-src http://<stream-mjpeg-url> [--fps N] To get stream URL of an IP camera, please refer to the user manual of your IP camera, or check the camera connection database . Get Nest Camera's Snapshot URL Follow the quick start guide to get snapshot URL. The high-level steps are: Create a \"product\" (the concept is like a project, we will use it for personal usage). Get access token from Nest cloud service. Get snapshot URL with the access token Nest cloud service. Single Image (for Development or Debugging) $ bn_camera --mode file --filepath <image-filepath>","title":"Camera"},{"location":"clients/camera/#usb-camera","text":"By default, camera client will use physically connected camera as input source: $ bn_camera You can use FPS parameter to control its image decoding rate: $ bn_camera --fps 5","title":"USB Camera"},{"location":"clients/camera/#rpi-camera","text":"The usage of RPi camera is the same as USB camera above. How to verify PiCamera HW","title":"RPi Camera"},{"location":"clients/camera/#ip-camera","text":"Before setting RTSP/MJPEG video stream as input source, I recommend to verify that the video stream can be played by a media player (e.g. VLC media player or Totem movie player). If input is RTSP video stream $ bn_camera --stream-src rtsp://<stream-rtsp-url> [--fps N] If input is MJPEG video stream $ bn_camera --stream-src http://<stream-mjpeg-url> [--fps N] To get stream URL of an IP camera, please refer to the user manual of your IP camera, or check the camera connection database .","title":"IP Camera"},{"location":"clients/camera/#get-nest-cameras-snapshot-url","text":"Follow the quick start guide to get snapshot URL. The high-level steps are: Create a \"product\" (the concept is like a project, we will use it for personal usage). Get access token from Nest cloud service. Get snapshot URL with the access token Nest cloud service.","title":"Get Nest Camera's Snapshot URL"},{"location":"clients/camera/#single-image-for-development-or-debugging","text":"$ bn_camera --mode file --filepath <image-filepath>","title":"Single Image (for Development or Debugging)"},{"location":"clients/dashboard/","text":"Dashboard is the place for displaying BerryNet's status, and the default dashboard is Freeboard which is a browser-based dashboard. Open Dashboard on RPi3 with Touchscreen Open browser and enter the URL: http://localhost/berrynet-dashboard Your screen will look like If your BerryNet is running the default detection engine, you can click LOAD FREEBOARD and choose the default configuration (or at <berrynet-src>/config/dashboard-tflitedetector.json ). After the configuration is loaded, Freeboard will look like Now your dashboard is ready to receive the inference results from BerryNet. Open Dashboard on Your Computer Open browser and enter the URL: http://<rpi-ip>/berrynet-dashboard Your screen will look like For the steps to load configuration, please refer to this section above. The only difference is that you need to change the MQTT SERVER of data source from localhost to RPi's IP (gateway IP). Click Detection Result and you can save the new configuration by clicking SAVE FREEBOARD . Frequent Questions Q1: I can not see anything on dashboard? A: Please follow the steps in this section . Q2: The status of the data sources look good, but I still can not see anything on dashboard? A: Please open an issue and share RPi's system log ( /var/log/syslog ) with us. We will help check the issue. Q3: Can BerryNet support any other dashboard? A: Yes, if a dashboard supports MQTT, BerryNet can support it technically. Please share your favorite dashboard with the community, and we can try to integrate it into BerryNet. Customization For more details about configuration (e.g. how to add widgets), please refer to Freeboard project .","title":"Dashboard"},{"location":"clients/dashboard/#open-dashboard-on-rpi3-with-touchscreen","text":"Open browser and enter the URL: http://localhost/berrynet-dashboard Your screen will look like If your BerryNet is running the default detection engine, you can click LOAD FREEBOARD and choose the default configuration (or at <berrynet-src>/config/dashboard-tflitedetector.json ). After the configuration is loaded, Freeboard will look like Now your dashboard is ready to receive the inference results from BerryNet.","title":"Open Dashboard on RPi3 with Touchscreen"},{"location":"clients/dashboard/#open-dashboard-on-your-computer","text":"Open browser and enter the URL: http://<rpi-ip>/berrynet-dashboard Your screen will look like For the steps to load configuration, please refer to this section above. The only difference is that you need to change the MQTT SERVER of data source from localhost to RPi's IP (gateway IP). Click Detection Result and you can save the new configuration by clicking SAVE FREEBOARD .","title":"Open Dashboard on Your Computer"},{"location":"clients/dashboard/#frequent-questions","text":"Q1: I can not see anything on dashboard? A: Please follow the steps in this section . Q2: The status of the data sources look good, but I still can not see anything on dashboard? A: Please open an issue and share RPi's system log ( /var/log/syslog ) with us. We will help check the issue. Q3: Can BerryNet support any other dashboard? A: Yes, if a dashboard supports MQTT, BerryNet can support it technically. Please share your favorite dashboard with the community, and we can try to integrate it into BerryNet.","title":"Frequent Questions"},{"location":"clients/dashboard/#customization","text":"For more details about configuration (e.g. how to add widgets), please refer to Freeboard project .","title":"Customization"},{"location":"clients/data_collector/","text":"Enable Data Collector Data collector ( bn_data_collector ) is useful if you want to store the snapshot and inference results for data analysis. Steps to enable data collector: Prepare config file including the topic where AI engine will send inference results to: For the default detection engine, you can create a data_collector.json { \"berrynet/engine/tflitedetector/result\": \"self.update\" } Run data collector: $ bn_data_collector --data-dirpath /tmp/bn-outdir --topic-config data_collector.json Check collected results in the indicated directory: Every inference result will be saved to two files: $ ls /tmp/bn-outdir 2019-05-26T13:33:52.277898.jpg 2019-05-26T13:33:52.277898.json ... image, w/ or w/o inference result overlay, depending on the output parameter set to AI engine. text in JSON format, w/o image string in base64 format.","title":"Data Collector"},{"location":"clients/data_collector/#enable-data-collector","text":"Data collector ( bn_data_collector ) is useful if you want to store the snapshot and inference results for data analysis. Steps to enable data collector: Prepare config file including the topic where AI engine will send inference results to: For the default detection engine, you can create a data_collector.json { \"berrynet/engine/tflitedetector/result\": \"self.update\" } Run data collector: $ bn_data_collector --data-dirpath /tmp/bn-outdir --topic-config data_collector.json Check collected results in the indicated directory: Every inference result will be saved to two files: $ ls /tmp/bn-outdir 2019-05-26T13:33:52.277898.jpg 2019-05-26T13:33:52.277898.json ... image, w/ or w/o inference result overlay, depending on the output parameter set to AI engine. text in JSON format, w/o image string in base64 format.","title":"Enable Data Collector"},{"location":"clients/email/","text":"Gmail Gmail client can send the targeting inference result to your Gmail. Pre-Condition There are two configuration requirements for your Gmail account: 2-Step Verification is Off . Less secure app access is On . You can check these two configurations from here . Usage For example, Alice has a people detection system made by BerryNet, and wants to use her Gmail to send a notification to Bob if there is any person is detected by the system. Alice sets a target label person : $ python3 gmail.py \\ --sender-address alice@gmail.com \\ --sender-password <sender-address-password> \\ --receiver-address bob@dt42.io \\ --target-label person","title":"Email"},{"location":"clients/email/#gmail","text":"Gmail client can send the targeting inference result to your Gmail.","title":"Gmail"},{"location":"clients/email/#pre-condition","text":"There are two configuration requirements for your Gmail account: 2-Step Verification is Off . Less secure app access is On . You can check these two configurations from here .","title":"Pre-Condition"},{"location":"clients/email/#usage","text":"For example, Alice has a people detection system made by BerryNet, and wants to use her Gmail to send a notification to Bob if there is any person is detected by the system. Alice sets a target label person : $ python3 gmail.py \\ --sender-address alice@gmail.com \\ --sender-password <sender-address-password> \\ --receiver-address bob@dt42.io \\ --target-label person","title":"Usage"},{"location":"clients/line/","text":"IMPORTANT: This page is being updated and the content might not be 100% correct. Description BerryNet LINE client helps send the texts and images of the inference results to your mobile LINE app. So you can receive instant notifications no matter where you are with Internet connection. Steps to setup LINE mobile app Create a LINE channel Follow the official document . Choose Messaging API channel. Add the channel as your LINE friend Scan the channel QR code by your LINE mobile app. Get 3 channel settings: user ID , channel secret , and channel access token In Channel settings, go to the Message settings section, and generate a Channel access token by clicking the issue button. BerryNet LINE client needs the settings to send results to this LINE channel. Steps to use BerryNet LINE client Add the 3 channel settings into BerryNet configuration file ( /usr/local/berrynet/config.js ) config.LINETargetUserID = 'your-user-id'; config.LINEChannelSecret = 'your-channel-secret'; config.LINEChannelAccessToken = 'your-channel-access-token'; Run BerryNet LINE client $ nodejs /usr/local/berrynet/line.js Now, you can receive BerryNet inference results from your LINE mobile app.","title":"LINE"},{"location":"clients/line/#description","text":"BerryNet LINE client helps send the texts and images of the inference results to your mobile LINE app. So you can receive instant notifications no matter where you are with Internet connection.","title":"Description"},{"location":"clients/line/#steps-to-setup-line-mobile-app","text":"Create a LINE channel Follow the official document . Choose Messaging API channel. Add the channel as your LINE friend Scan the channel QR code by your LINE mobile app. Get 3 channel settings: user ID , channel secret , and channel access token In Channel settings, go to the Message settings section, and generate a Channel access token by clicking the issue button. BerryNet LINE client needs the settings to send results to this LINE channel.","title":"Steps to setup LINE mobile app"},{"location":"clients/line/#steps-to-use-berrynet-line-client","text":"Add the 3 channel settings into BerryNet configuration file ( /usr/local/berrynet/config.js ) config.LINETargetUserID = 'your-user-id'; config.LINEChannelSecret = 'your-channel-secret'; config.LINEChannelAccessToken = 'your-channel-access-token'; Run BerryNet LINE client $ nodejs /usr/local/berrynet/line.js Now, you can receive BerryNet inference results from your LINE mobile app.","title":"Steps to use BerryNet LINE client"},{"location":"clients/telegram/","text":"You can receive the notification images (image with classification or detection results) in your Telegram app. Create Your Telegram Bot and Get Its Access Token. Install a Telegram app on your mobile or desktop. Search @botfather and start a conversation. Send /newbot . Enter the bot name\uff0cthe end of the name has to be bot . E.g., bntest1_bot . BotFather will sends a message including: Bot link: You can start a conversation with the bot by clicking the link. HTTP API Token (Access Token): Write down the token. It is necessary to run the BerryNet Telegram client. Get The Chat ID (skip the step currently). Send a message to the bot. Open the URL https://api.telegram.org/bot<yourtoken>/getUpdates . You will find the chat ID in the returned data. Run BerryNet Telegram Client. $ bn_telegram --token <token> The token parameter can be a token string or a token filepath: Token string Example: 1234567890:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghi The filepath of the configuration file Any filepath is okay and we suggest $HOME/.config/berrynet/telegram.json . Configuration file format $ cat $HOME/.config/berrynet/telegram.json { \"token\": \"<telegram-token>\" } Note: bn_telegram is not in the latest system image. You can download the source code and run python3 telegram_bot.py ... before we update the system image. Send /camera to your bot. If you receive the message: Dear, I am ready to help send notification You will receive notification images in your mobile/desktop Telegram app.","title":"Telegram"},{"location":"community/donation/","text":"One-Time Donations We accept donations through these channels: Crypto Wallet Address bc1qnynxdh8nlyvkgfvyw3fl22yfcvrkmqzscg2k6f 0x88659538ABb056E106683F31F8FBc2f0F0aDa136 bnb1ewlh3gequz5w3ewy5urj952rs5l0kfzh5tphkj Website Link https://www.paypal.me/berrynet Recurring Pledges Become a backer or sponsor on Open Collective . Why Donate First, thank you for your generosity and for reading this page. BerryNet is a 100% FLOSS project, and we appreciate the positive feedback from the community. It encourages DT42 to keep believing in free software and keep our best to put engineering resources into software development. Currently, the BerryNet project members cover the cost of infrastructure operation, travel, and time to share in the conferences, talks, and education events. Your generosity can help us keep democratize edge AI technologies to the world. How The Donation Will Be Used We promise that we will only use the donation in: Infrastructure operation. Public knowledge-sharing events. We will make the receipts of every cost public if its financial support comes from the donation. Bounties for Freedom If you think that BerryNet will be even more helpful for the world to have some features and you are willing to provide financial support, please feel free to contact us or open a feature request on the GitHub issue. We promise that the working outputs will always be open and available to everyone. Backer List Your name will be shown on the backer list by default. If you do not want to be listed, please send your name to bofu AT dt42.io and we will update the lis. Thank you for believing in free software together. The BerryNet team.","title":"Donation"},{"location":"community/donation/#one-time-donations","text":"We accept donations through these channels: Crypto Wallet Address bc1qnynxdh8nlyvkgfvyw3fl22yfcvrkmqzscg2k6f 0x88659538ABb056E106683F31F8FBc2f0F0aDa136 bnb1ewlh3gequz5w3ewy5urj952rs5l0kfzh5tphkj Website Link https://www.paypal.me/berrynet","title":"One-Time Donations"},{"location":"community/donation/#recurring-pledges","text":"Become a backer or sponsor on Open Collective .","title":"Recurring Pledges"},{"location":"community/donation/#why-donate","text":"First, thank you for your generosity and for reading this page. BerryNet is a 100% FLOSS project, and we appreciate the positive feedback from the community. It encourages DT42 to keep believing in free software and keep our best to put engineering resources into software development. Currently, the BerryNet project members cover the cost of infrastructure operation, travel, and time to share in the conferences, talks, and education events. Your generosity can help us keep democratize edge AI technologies to the world.","title":"Why Donate"},{"location":"community/donation/#how-the-donation-will-be-used","text":"We promise that we will only use the donation in: Infrastructure operation. Public knowledge-sharing events. We will make the receipts of every cost public if its financial support comes from the donation.","title":"How The Donation Will Be Used"},{"location":"community/donation/#bounties-for-freedom","text":"If you think that BerryNet will be even more helpful for the world to have some features and you are willing to provide financial support, please feel free to contact us or open a feature request on the GitHub issue. We promise that the working outputs will always be open and available to everyone.","title":"Bounties for Freedom"},{"location":"community/donation/#backer-list","text":"Your name will be shown on the backer list by default. If you do not want to be listed, please send your name to bofu AT dt42.io and we will update the lis. Thank you for believing in free software together. The BerryNet team.","title":"Backer List"},{"location":"community/qa/","text":"There are 2 places for interaction with the community: GitHub Issues For issue report. Please always feel free to report issues, and we always appreciate your time and your interest in BerryNet. Teleram Group and Google Group For non-issue questions, like programming issues, architectural questions, suggestions and feedback, etc.","title":"Troubleshooting and Questions"},{"location":"community/repository/","text":"What is BerryNet Repository Our goal is to push BerryNet SW components into Debian repository as many as possible. Before the process is completed, or some components will not be accepted because of Debian policy violation (e.g. close sourced OpenVINO SDK), you can install these SW components from BerryNet repository . BerryNet repository will be set automatically in the installation process. To configure BerryNet repository manually, please refer to BerryNet repository . Besides of BerryNet itself, there are many useful packages: ML/DL models AI frameworks Pre-built OpenCV Freeboard","title":"Repository"},{"location":"community/repository/#what-is-berrynet-repository","text":"Our goal is to push BerryNet SW components into Debian repository as many as possible. Before the process is completed, or some components will not be accepted because of Debian policy violation (e.g. close sourced OpenVINO SDK), you can install these SW components from BerryNet repository . BerryNet repository will be set automatically in the installation process. To configure BerryNet repository manually, please refer to BerryNet repository . Besides of BerryNet itself, there are many useful packages: ML/DL models AI frameworks Pre-built OpenCV Freeboard","title":"What is BerryNet Repository"},{"location":"deep_learning/dlmodelbox/","text":"DLModelBox https://github.com/DT42/DLModelBox","title":"Model Management"},{"location":"deep_learning/dlmodelbox/#dlmodelbox","text":"https://github.com/DT42/DLModelBox","title":"DLModelBox"},{"location":"deep_learning/transfer_learning/","text":"Use Your Data To Train The original instruction of retraining YOLOv2 model see github repository of darknet In the current of BerryNet, TinyYolo is used instead of YOLOv2. The major differences are: Create file yolo-obj.cfg with the same content as in tiny-yolo.cfg Download pre-trained weights of darknet reference model, darknet.weights.12 , for the convolutional layers (6.1MB) The rest parts are the same as retraining YOLO. If you use LabelMe to annotate data, utils/xmlTotxt.py can help convert the xml format to the text format that darknet uses.","title":"Transfer Learning"},{"location":"deep_learning/transfer_learning/#use-your-data-to-train","text":"The original instruction of retraining YOLOv2 model see github repository of darknet In the current of BerryNet, TinyYolo is used instead of YOLOv2. The major differences are: Create file yolo-obj.cfg with the same content as in tiny-yolo.cfg Download pre-trained weights of darknet reference model, darknet.weights.12 , for the convolutional layers (6.1MB) The rest parts are the same as retraining YOLO. If you use LabelMe to annotate data, utils/xmlTotxt.py can help convert the xml format to the text format that darknet uses.","title":"Use Your Data To Train"},{"location":"developer/architecture/","text":"NOTE: This page is not completed yet Overview Work in Progress Communication MQTT topic format is based on the concept: berrynet/<component-type>/<component-name>/<message-type>[/message-additional-info...] MQTT topics using in BerryNet AI services berrynet/engine/darknet/result berrynet/engine/ovclassifier/result berrynet/engine/ovdetector/result berrynet/engine/pipeline/result berrynet/engine/tensorflow/result berrynet/engine/tfliteclassifier/result berrynet/engine/tflitedetector/result Clients berrynet/data/rgbimage berrynet/dashboard/snapshot berrynet/dashboard/inferenceResult Generic Inference Result Format Generic inference result format makes AI inference services to follow the same rule. Classification { \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"classification\", \"label\": STRING, \"confidence\": FLOAT32 }, ... ] } Detection { \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"detection\", \"label\": STRING, \"confidence\": FLOAT32, \"top\": UINT32, \"bottom\": UINT32, \"left\": UINT32, \"right\", UINT32, }, ... ] } Field Description Field Description Type timestamp Datetime string in ISO format. STRING bytes Raw image or image with inference results in base64 format. STRING type Inference type. Valid values: {classification, detection} STRING label Inference result label. STRING confidence Inference result confidence. Valid value: 0.00 <= confidence <= 1.00 FLOAT32 left x of the top-left point. UINT32 top y of the top-left point. UINT32 right x of the bottom-right point. UINT32 bottom y of the bottom-right point. UINT32","title":"Architecture"},{"location":"developer/architecture/#overview","text":"Work in Progress","title":"Overview"},{"location":"developer/architecture/#communication","text":"MQTT topic format is based on the concept: berrynet/<component-type>/<component-name>/<message-type>[/message-additional-info...] MQTT topics using in BerryNet AI services berrynet/engine/darknet/result berrynet/engine/ovclassifier/result berrynet/engine/ovdetector/result berrynet/engine/pipeline/result berrynet/engine/tensorflow/result berrynet/engine/tfliteclassifier/result berrynet/engine/tflitedetector/result Clients berrynet/data/rgbimage berrynet/dashboard/snapshot berrynet/dashboard/inferenceResult","title":"Communication"},{"location":"developer/architecture/#generic-inference-result-format","text":"Generic inference result format makes AI inference services to follow the same rule.","title":"Generic Inference Result Format"},{"location":"developer/architecture/#classification","text":"{ \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"classification\", \"label\": STRING, \"confidence\": FLOAT32 }, ... ] }","title":"Classification"},{"location":"developer/architecture/#detection","text":"{ \"timestamp\": STRING, \"bytes\": STRING, \"annotations\": [ { \"type\": \"detection\", \"label\": STRING, \"confidence\": FLOAT32, \"top\": UINT32, \"bottom\": UINT32, \"left\": UINT32, \"right\", UINT32, }, ... ] }","title":"Detection"},{"location":"developer/architecture/#field-description","text":"Field Description Type timestamp Datetime string in ISO format. STRING bytes Raw image or image with inference results in base64 format. STRING type Inference type. Valid values: {classification, detection} STRING label Inference result label. STRING confidence Inference result confidence. Valid value: 0.00 <= confidence <= 1.00 FLOAT32 left x of the top-left point. UINT32 top y of the top-left point. UINT32 right x of the bottom-right point. UINT32 bottom y of the bottom-right point. UINT32","title":"Field Description"},{"location":"engines/darknet/","text":"Installation Run the install_darknet function in configure . BerryNet dev team is making this to be easier and clearer. Test Darknet Engine Manually Use Darknet detector service and tinyyolovoc model package as example. Start detector service $ bn_darknet \\ -p tinyyolovoc-20170816 \\ --service_name darknet \\ --num_threads 4 \\ --draw \\ --debug [D 191206 21:20:42 darknet_service:109] model filepath: /usr/share/dlmodels/tinyyolovoc-20170816/tiny-yolo-voc.weights [D 191206 21:20:42 darknet_service:110] label filepath: /usr/share/dlmodels/tinyyolovoc-20170816/voc.names layer filters size input output 0 conv 16 3 x 3 / 1 416 x 416 x 3 -> 416 x 416 x 16 1 max 2 x 2 / 2 416 x 416 x 16 -> 208 x 208 x 16 2 conv 32 3 x 3 / 1 208 x 208 x 16 -> 208 x 208 x 32 3 max 2 x 2 / 2 208 x 208 x 32 -> 104 x 104 x 32 4 conv 64 3 x 3 / 1 104 x 104 x 32 -> 104 x 104 x 64 5 max 2 x 2 / 2 104 x 104 x 64 -> 52 x 52 x 64 6 conv 128 3 x 3 / 1 52 x 52 x 64 -> 52 x 52 x 128 7 max 2 x 2 / 2 52 x 52 x 128 -> 26 x 26 x 128 8 conv 256 3 x 3 / 1 26 x 26 x 128 -> 26 x 26 x 256 9 max 2 x 2 / 2 26 x 26 x 256 -> 13 x 13 x 256 10 conv 512 3 x 3 / 1 13 x 13 x 256 -> 13 x 13 x 512 11 max 2 x 2 / 1 13 x 13 x 512 -> 13 x 13 x 512 12 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 13 conv 1024 3 x 3 / 1 13 x 13 x1024 -> 13 x 13 x1024 14 conv 125 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 125 15 detection mask_scale: Using default '1.000000' Loading weights from /usr/share/dlmodels/tinyyolovoc-20170816/tiny-yolo-voc.weights...Done! [D 191206 21:20:45 darknet_engine:150] inference time: 1.9036920070648193 s [D 191206 21:20:45 __init__:11] Connected with result code 0 [D 191206 21:20:45 __init__:13] Subscribe topic berrynet/data/rgbimage Send image to service by camera client $ bn_camera --mode file --filepath <image-filepath> You should see the inference results on detector's terminal [D 191206 21:21:45 __init__:20] Receive message from topic berrynet/data/rgbimage [D 191206 21:21:47 darknet_engine:150] inference time: 1.8199963569641113 s [D 191206 21:21:47 darknet_service:66] result_hook, annotations: [{'bottom': 342.45084381103516, 'confidence': 0.48404842615127563, 'left': 56.02063751220703, 'label': 'dog', 'right': 198.14881134033203, 'id': -1, 'top': 265.13106536865234, 'type': 'detection'}, {'bottom': 350.23523712158203, 'confidence': 0.32306814193725586, 'left': 107.26636505126953, 'label': 'dog', 'right': 225.97444915771484, 'id': -1, 'top': 275.51012420654297, 'type': 'detection'}, {'bottom': 360.5461120605469, 'confidence': 0.7337859272956848, 'left': 169.8051300048828, 'label': 'person', 'right': 280.67076110839844, 'id': -1, 'top': 94.009033203125, 'type': 'detection'}, {'bottom': 346.7984390258789, 'confidence': 0.6094380021095276, 'left': 412.9965133666992, 'label': 'sheep', 'right': 541.2455520629883, 'id': -1, 'top': 156.6410140991211, 'type': 'detection'}, {'bottom': 359.8292579650879, 'confidence': 0.39942821860313416, 'left': 69.68604278564453, 'label': 'sheep', 'right': 191.7149887084961, 'id': -1, 'top': 254.3123435974121, 'type': 'detection'}] [D 191206 21:21:47 __init__:50] Send message to topic berrynet/engine/darknet/result To visualize the received inference result $ bn_dashboard --no-decoration --no-full-screen --topic berrynet/engine/darknet/result --debug","title":"Darknet"},{"location":"engines/darknet/#installation","text":"Run the install_darknet function in configure . BerryNet dev team is making this to be easier and clearer.","title":"Installation"},{"location":"engines/darknet/#test-darknet-engine-manually","text":"Use Darknet detector service and tinyyolovoc model package as example. Start detector service $ bn_darknet \\ -p tinyyolovoc-20170816 \\ --service_name darknet \\ --num_threads 4 \\ --draw \\ --debug [D 191206 21:20:42 darknet_service:109] model filepath: /usr/share/dlmodels/tinyyolovoc-20170816/tiny-yolo-voc.weights [D 191206 21:20:42 darknet_service:110] label filepath: /usr/share/dlmodels/tinyyolovoc-20170816/voc.names layer filters size input output 0 conv 16 3 x 3 / 1 416 x 416 x 3 -> 416 x 416 x 16 1 max 2 x 2 / 2 416 x 416 x 16 -> 208 x 208 x 16 2 conv 32 3 x 3 / 1 208 x 208 x 16 -> 208 x 208 x 32 3 max 2 x 2 / 2 208 x 208 x 32 -> 104 x 104 x 32 4 conv 64 3 x 3 / 1 104 x 104 x 32 -> 104 x 104 x 64 5 max 2 x 2 / 2 104 x 104 x 64 -> 52 x 52 x 64 6 conv 128 3 x 3 / 1 52 x 52 x 64 -> 52 x 52 x 128 7 max 2 x 2 / 2 52 x 52 x 128 -> 26 x 26 x 128 8 conv 256 3 x 3 / 1 26 x 26 x 128 -> 26 x 26 x 256 9 max 2 x 2 / 2 26 x 26 x 256 -> 13 x 13 x 256 10 conv 512 3 x 3 / 1 13 x 13 x 256 -> 13 x 13 x 512 11 max 2 x 2 / 1 13 x 13 x 512 -> 13 x 13 x 512 12 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 13 conv 1024 3 x 3 / 1 13 x 13 x1024 -> 13 x 13 x1024 14 conv 125 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 125 15 detection mask_scale: Using default '1.000000' Loading weights from /usr/share/dlmodels/tinyyolovoc-20170816/tiny-yolo-voc.weights...Done! [D 191206 21:20:45 darknet_engine:150] inference time: 1.9036920070648193 s [D 191206 21:20:45 __init__:11] Connected with result code 0 [D 191206 21:20:45 __init__:13] Subscribe topic berrynet/data/rgbimage Send image to service by camera client $ bn_camera --mode file --filepath <image-filepath> You should see the inference results on detector's terminal [D 191206 21:21:45 __init__:20] Receive message from topic berrynet/data/rgbimage [D 191206 21:21:47 darknet_engine:150] inference time: 1.8199963569641113 s [D 191206 21:21:47 darknet_service:66] result_hook, annotations: [{'bottom': 342.45084381103516, 'confidence': 0.48404842615127563, 'left': 56.02063751220703, 'label': 'dog', 'right': 198.14881134033203, 'id': -1, 'top': 265.13106536865234, 'type': 'detection'}, {'bottom': 350.23523712158203, 'confidence': 0.32306814193725586, 'left': 107.26636505126953, 'label': 'dog', 'right': 225.97444915771484, 'id': -1, 'top': 275.51012420654297, 'type': 'detection'}, {'bottom': 360.5461120605469, 'confidence': 0.7337859272956848, 'left': 169.8051300048828, 'label': 'person', 'right': 280.67076110839844, 'id': -1, 'top': 94.009033203125, 'type': 'detection'}, {'bottom': 346.7984390258789, 'confidence': 0.6094380021095276, 'left': 412.9965133666992, 'label': 'sheep', 'right': 541.2455520629883, 'id': -1, 'top': 156.6410140991211, 'type': 'detection'}, {'bottom': 359.8292579650879, 'confidence': 0.39942821860313416, 'left': 69.68604278564453, 'label': 'sheep', 'right': 191.7149887084961, 'id': -1, 'top': 254.3123435974121, 'type': 'detection'}] [D 191206 21:21:47 __init__:50] Send message to topic berrynet/engine/darknet/result To visualize the received inference result $ bn_dashboard --no-decoration --no-full-screen --topic berrynet/engine/darknet/result --debug","title":"Test Darknet Engine Manually"},{"location":"engines/movidius/","text":"IMPORTANT: This page is being updated and the content might not be 100% correct. Movidius neural compute stick (NCS) is an USB device containing Myriad 2, the DLA (Deep Learning Accelerator) chip. Feature Description Inference Performance FPS 7.5 for object detection (MobileNet SSD) Hardware Performance 80~150 GFLOPS [1] Power Consumption 1W power envelope [2] BerryNet Movidius Engine helps user to create edge AIoT applications in minutes, and is powered by OpenVINO . RPi3 w/ 2-NCS UP Squared (w/ Myriad 2) Enable Movidius Engine Here are the steps to enable the support: Install dependencies and inference engine: $ bash utils/install-movidius.sh Update BerryNet inference engine settings Edit /usr/local/bin/berrynet-manager Replace detection_fast_server.service by classify_movidius_server.service Edit /usr/local/berrynet/config.js Set config.inferenceEngine from detector to classifier","title":"Movidius"},{"location":"engines/movidius/#enable-movidius-engine","text":"Here are the steps to enable the support: Install dependencies and inference engine: $ bash utils/install-movidius.sh Update BerryNet inference engine settings Edit /usr/local/bin/berrynet-manager Replace detection_fast_server.service by classify_movidius_server.service Edit /usr/local/berrynet/config.js Set config.inferenceEngine from detector to classifier","title":"Enable Movidius Engine"},{"location":"engines/openvino/","text":"Installation Setup BerryNet repository Install OpenVINO packages # Ubuntu xenial and bionic $ sudo apt-get install openvino # Raspbian $ sudo apt-get install openvino-rpi (Optional) Install example classification or detection models for Intel NCS2 (FP16 model) # classification $ sudo apt-get install mobilenet-1.0-224-fp16-openvino # detection $ sudo apt-get install mobilenet-ssd-openvino Setup Initialize OpenVINO execution environment Add this line below into shell's config file, e.g. $HOME/.bashrc , or run it manually in your console every time: $ source /opt/intel/openvino_2019.1.144/bin/setupvars.sh Test OpenVINO Engine Manually Use OpenVINO detector service and mobilenet-ssd-openvino model package as example. Insert NCS2 to your computer. Start detector service $ source /opt/intel/openvino_2019.1.144/bin/setupvars.sh $ python3 /usr/lib/python3/dist-packages/berrynet/service/openvino_service.py \\ --service detector \\ --model /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.xml \\ --label /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/labels.txt \\ --service_name ovdetector \\ -d MYRIAD \\ --draw \\ --debug [D 190617 21:30:13 openvino_service:177] model filepath: /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.xml [D 190617 21:30:13 openvino_service:178] label filepath: /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/labels.txt [D 190617 21:30:13 openvino_engine:167] Loading network files: /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.xml /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.bin [D 190617 21:30:13 openvino_engine:184] Preparing input blobs [D 190617 21:30:15 __init__:11] Connected with result code 0 [D 190617 21:30:15 __init__:13] Subscribe topic berrynet/data/rgbimage Send image to service by camera client $ bn_camera --mode file --filepath <image-filepath> You should see the inference results on detector's terminal [D 190617 21:31:34 __init__:20] Receive message from topic berrynet/data/rgbimage [D 190617 21:31:34 openvino_service:95] payload size: 162264 [D 190617 21:31:34 openvino_service:96] payload type: <class 'bytes'> [D 190617 21:31:34 openvino_service:99] destringify_jpg: 1.29 ms [D 190617 21:31:34 openvino_service:103] jpg2bgr: 5.436 ms [D 190617 21:31:34 openvino_engine:241] Inference time: 37.625 ms [D 190617 21:31:34 openvino_engine:257] Processing output blob [D 190617 21:31:34 openvino_engine:258] Threshold: 0.3 [D 190617 21:31:34 openvino_service:109] Result: {'annotations': [{'top': 261, 'left': 70, 'label': 'dog', 'right': 200, 'bottom': 345, 'confidence': 0.7890625}, {'top': 138, 'left': 398, 'label': 'horse', 'right': 602, 'bottom': 336, 'confidence': 0.3623046875}, {'top': 104, 'left': 174, 'label': 'person', 'right': 274, 'bottom': 366, 'confidence': 0.9931640625}, {'top': 136, 'left': 404, 'label': 'sheep', 'right': 606, 'bottom': 335, 'confidence': 0.6171875}]} [D 190617 21:31:34 openvino_service:110] Detection takes 43.864 ms [D 190617 21:31:34 openvino_service:115] draw = True [D 190617 21:31:34 openvino_service:126] result_hook, annotations: [{'top': 261, 'left': 70, 'label': 'dog', 'right': 200, 'bottom': 345, 'confidence': 0.7890625}, {'top': 138, 'left': 398, 'label': 'horse', 'right': 602, 'bottom': 336, 'confidence': 0.3623046875}, {'top': 104, 'left': 174, 'label': 'person', 'right': 274, 'bottom': 366, 'confidence': 0.9931640625}, {'top': 136, 'left': 404, 'label': 'sheep', 'right': 606, 'bottom': 335, 'confidence': 0.6171875}] [D 190617 21:31:34 __init__:50] Send message to topic berrynet/engine/ovdetector/result","title":"OpenVINO"},{"location":"engines/openvino/#installation","text":"Setup BerryNet repository Install OpenVINO packages # Ubuntu xenial and bionic $ sudo apt-get install openvino # Raspbian $ sudo apt-get install openvino-rpi (Optional) Install example classification or detection models for Intel NCS2 (FP16 model) # classification $ sudo apt-get install mobilenet-1.0-224-fp16-openvino # detection $ sudo apt-get install mobilenet-ssd-openvino","title":"Installation"},{"location":"engines/openvino/#setup","text":"Initialize OpenVINO execution environment Add this line below into shell's config file, e.g. $HOME/.bashrc , or run it manually in your console every time: $ source /opt/intel/openvino_2019.1.144/bin/setupvars.sh","title":"Setup"},{"location":"engines/openvino/#test-openvino-engine-manually","text":"Use OpenVINO detector service and mobilenet-ssd-openvino model package as example. Insert NCS2 to your computer. Start detector service $ source /opt/intel/openvino_2019.1.144/bin/setupvars.sh $ python3 /usr/lib/python3/dist-packages/berrynet/service/openvino_service.py \\ --service detector \\ --model /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.xml \\ --label /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/labels.txt \\ --service_name ovdetector \\ -d MYRIAD \\ --draw \\ --debug [D 190617 21:30:13 openvino_service:177] model filepath: /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.xml [D 190617 21:30:13 openvino_service:178] label filepath: /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/labels.txt [D 190617 21:30:13 openvino_engine:167] Loading network files: /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.xml /usr/share/dlmodels/mobilenet-ssd-openvino-1.0.0/mobilenet-ssd.bin [D 190617 21:30:13 openvino_engine:184] Preparing input blobs [D 190617 21:30:15 __init__:11] Connected with result code 0 [D 190617 21:30:15 __init__:13] Subscribe topic berrynet/data/rgbimage Send image to service by camera client $ bn_camera --mode file --filepath <image-filepath> You should see the inference results on detector's terminal [D 190617 21:31:34 __init__:20] Receive message from topic berrynet/data/rgbimage [D 190617 21:31:34 openvino_service:95] payload size: 162264 [D 190617 21:31:34 openvino_service:96] payload type: <class 'bytes'> [D 190617 21:31:34 openvino_service:99] destringify_jpg: 1.29 ms [D 190617 21:31:34 openvino_service:103] jpg2bgr: 5.436 ms [D 190617 21:31:34 openvino_engine:241] Inference time: 37.625 ms [D 190617 21:31:34 openvino_engine:257] Processing output blob [D 190617 21:31:34 openvino_engine:258] Threshold: 0.3 [D 190617 21:31:34 openvino_service:109] Result: {'annotations': [{'top': 261, 'left': 70, 'label': 'dog', 'right': 200, 'bottom': 345, 'confidence': 0.7890625}, {'top': 138, 'left': 398, 'label': 'horse', 'right': 602, 'bottom': 336, 'confidence': 0.3623046875}, {'top': 104, 'left': 174, 'label': 'person', 'right': 274, 'bottom': 366, 'confidence': 0.9931640625}, {'top': 136, 'left': 404, 'label': 'sheep', 'right': 606, 'bottom': 335, 'confidence': 0.6171875}]} [D 190617 21:31:34 openvino_service:110] Detection takes 43.864 ms [D 190617 21:31:34 openvino_service:115] draw = True [D 190617 21:31:34 openvino_service:126] result_hook, annotations: [{'top': 261, 'left': 70, 'label': 'dog', 'right': 200, 'bottom': 345, 'confidence': 0.7890625}, {'top': 138, 'left': 398, 'label': 'horse', 'right': 602, 'bottom': 336, 'confidence': 0.3623046875}, {'top': 104, 'left': 174, 'label': 'person', 'right': 274, 'bottom': 366, 'confidence': 0.9931640625}, {'top': 136, 'left': 404, 'label': 'sheep', 'right': 606, 'bottom': 335, 'confidence': 0.6171875}] [D 190617 21:31:34 __init__:50] Send message to topic berrynet/engine/ovdetector/result","title":"Test OpenVINO Engine Manually"},{"location":"engines/tflite/","text":"Engine: TensorFlow Lite Demo Video Installation Run the install_tensorflow function in configure . BerryNet dev team is making this to be easier and clearer. Test TFLite Engine Manually Use TFLite detector service and mobilenet-ssd-coco-tflite model package as example. Start detector service $ python3 tflite_service.py \\ --service detector \\ --model /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/model.tflite \\ --label /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/labels.txt \\ --service_name tfdetector \\ --num_threads 4 \\ --draw \\ --debug /usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7 return f(*args, **kwds) [D 190711 11:50:17 tflite_service:174] model filepath: /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/model.tflite [D 190711 11:50:17 tflite_service:175] label filepath: /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/labels.txt INFO: Initialized TensorFlow Lite runtime. [D 190711 11:50:17 __init__:11] Connected with result code 0 [D 190711 11:50:17 __init__:13] Subscribe topic berrynet/data/rgbimage Send image to service by camera client $ bn_camera --mode file --filepath <image-filepath> Example image: wget https://github.com/pjreddie/darknet/raw/master/data/dog.jpg You should see the inference results on detector's terminal [D 190705 15:39:20 __init__:20] Receive message from topic berrynet/data/rgbimage [D 190705 15:39:20 tflite_service:46] payload size: 161232 [D 190705 15:39:20 tflite_service:47] payload type: <class 'bytes'> [D 190705 15:39:20 tflite_service:50] destringify_jpg: 4.505 ms [D 190705 15:39:20 tflite_service:54] jpg2bgr: 10.444 ms [D 190705 15:39:21 tflite_service:60] Result: {'annotations': [{'label': 'person', 'confidence': 0.9751802682876587, 'left': 187, 'top': 99, 'right': 273, 'bottom': 384}, {'label': 'sheep', 'confidence': 0.9385143518447876, 'left': 401, 'top': 135, 'right': 600, 'bottom': 339}, {'label': 'dog', 'confidence': 0.7179926037788391, 'left': 68, 'top': 263, 'right': 200, 'bottom': 349}]} [D 190705 15:39:21 tflite_service:61] Detection takes 527.504 ms [D 190705 15:39:21 tflite_service:66] draw = True [D 190705 15:39:21 tflite_service:77] result_hook, annotations: [{'label': 'person', 'confidence': 0.9751802682876587, 'left': 187, 'top': 99, 'right': 273, 'bottom': 384}, {'label': 'sheep', 'confidence': 0.9385143518447876, 'left': 401, 'top': 135, 'right': 600, 'bottom': 339}, {'label': 'dog', 'confidence': 0.7179926037788391, 'left': 68, 'top': 263, 'right': 200, 'bottom': 349}] [D 190705 15:39:21 __init__:50] Send message to topic berrynet/engine/tflitedetector/result To visualize the received inference result $ bn_dashboard --no-decoration --no-full-screen --topic berrynet/engine/tflitedetector/result --debug","title":"TensorFlow Lite"},{"location":"engines/tflite/#engine-tensorflow-lite","text":"","title":"Engine: TensorFlow Lite"},{"location":"engines/tflite/#demo-video","text":"","title":"Demo Video"},{"location":"engines/tflite/#installation","text":"Run the install_tensorflow function in configure . BerryNet dev team is making this to be easier and clearer.","title":"Installation"},{"location":"engines/tflite/#test-tflite-engine-manually","text":"Use TFLite detector service and mobilenet-ssd-coco-tflite model package as example. Start detector service $ python3 tflite_service.py \\ --service detector \\ --model /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/model.tflite \\ --label /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/labels.txt \\ --service_name tfdetector \\ --num_threads 4 \\ --draw \\ --debug /usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7 return f(*args, **kwds) [D 190711 11:50:17 tflite_service:174] model filepath: /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/model.tflite [D 190711 11:50:17 tflite_service:175] label filepath: /usr/share/dlmodels/mobilenet-ssd-coco-tflite-2.0.0/labels.txt INFO: Initialized TensorFlow Lite runtime. [D 190711 11:50:17 __init__:11] Connected with result code 0 [D 190711 11:50:17 __init__:13] Subscribe topic berrynet/data/rgbimage Send image to service by camera client $ bn_camera --mode file --filepath <image-filepath> Example image: wget https://github.com/pjreddie/darknet/raw/master/data/dog.jpg You should see the inference results on detector's terminal [D 190705 15:39:20 __init__:20] Receive message from topic berrynet/data/rgbimage [D 190705 15:39:20 tflite_service:46] payload size: 161232 [D 190705 15:39:20 tflite_service:47] payload type: <class 'bytes'> [D 190705 15:39:20 tflite_service:50] destringify_jpg: 4.505 ms [D 190705 15:39:20 tflite_service:54] jpg2bgr: 10.444 ms [D 190705 15:39:21 tflite_service:60] Result: {'annotations': [{'label': 'person', 'confidence': 0.9751802682876587, 'left': 187, 'top': 99, 'right': 273, 'bottom': 384}, {'label': 'sheep', 'confidence': 0.9385143518447876, 'left': 401, 'top': 135, 'right': 600, 'bottom': 339}, {'label': 'dog', 'confidence': 0.7179926037788391, 'left': 68, 'top': 263, 'right': 200, 'bottom': 349}]} [D 190705 15:39:21 tflite_service:61] Detection takes 527.504 ms [D 190705 15:39:21 tflite_service:66] draw = True [D 190705 15:39:21 tflite_service:77] result_hook, annotations: [{'label': 'person', 'confidence': 0.9751802682876587, 'left': 187, 'top': 99, 'right': 273, 'bottom': 384}, {'label': 'sheep', 'confidence': 0.9385143518447876, 'left': 401, 'top': 135, 'right': 600, 'bottom': 339}, {'label': 'dog', 'confidence': 0.7179926037788391, 'left': 68, 'top': 263, 'right': 200, 'bottom': 349}] [D 190705 15:39:21 __init__:50] Send message to topic berrynet/engine/tflitedetector/result To visualize the received inference result $ bn_dashboard --no-decoration --no-full-screen --topic berrynet/engine/tflitedetector/result --debug","title":"Test TFLite Engine Manually"},{"location":"tutorials/configuration/","text":"Default Configuration File Default configuration is installed in /etc/supervisor/conf.d/berrynet-tflite.conf . Configuration describes what are the services consisting of the AI application. Configuration examples are available in <berrynet-dir>/config/supervisor/conf.d/ . Raspberry Pi Configurations If you install BerryNet by image, RPi configurations will have been set. If you install BerryNet from source, please configure RPi by the steps: Open raspi-config in terminal rpi $ sudo raspi-config Enable RPi camera 5 Interfacing Options `-- P1 Camera `-- Yes Enable GL Driver 7 Advanced Options `-- A7 GL Driver `-- G2 GL (Fake KMS) (Optional) Enable SSH 5 Interfacing Options `-- P2 SSH `-- Yes (Optional) Change locale from UK to US 4 Localisation Options `-- I1 Change Locale `-- Cancel en_GB.UTF-8 and select en_US.UTF-8 `-- Choose en_US.UTF-8 as default locale (Optional) Change keyboard layout from UK to US 4 Localisation Options `-- I3 Change Keyboard Layout `-- Generic 105-key (Intl) PC `-- English (US) `-- The default for the keyboard layout `-- No compose key `-- No X server termination hotkey Wi-Fi Configuration To configure Wi-Fi manually, there are two methods: Use the raspi-config command and the GUI interface. Edit /etc/wpa_supplicant/wpa_supplicant.conf . A working example looks like below: $ cat wpa_supplicant.conf ctrl_interface=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=TW network={ ssid=\"MyAP\" psk=\"B3rryN3t\" key_mgmt=WPA-PSK }","title":"Configuration"},{"location":"tutorials/configuration/#default-configuration-file","text":"Default configuration is installed in /etc/supervisor/conf.d/berrynet-tflite.conf . Configuration describes what are the services consisting of the AI application. Configuration examples are available in <berrynet-dir>/config/supervisor/conf.d/ .","title":"Default Configuration File"},{"location":"tutorials/configuration/#raspberry-pi-configurations","text":"If you install BerryNet by image, RPi configurations will have been set. If you install BerryNet from source, please configure RPi by the steps: Open raspi-config in terminal rpi $ sudo raspi-config Enable RPi camera 5 Interfacing Options `-- P1 Camera `-- Yes Enable GL Driver 7 Advanced Options `-- A7 GL Driver `-- G2 GL (Fake KMS) (Optional) Enable SSH 5 Interfacing Options `-- P2 SSH `-- Yes (Optional) Change locale from UK to US 4 Localisation Options `-- I1 Change Locale `-- Cancel en_GB.UTF-8 and select en_US.UTF-8 `-- Choose en_US.UTF-8 as default locale (Optional) Change keyboard layout from UK to US 4 Localisation Options `-- I3 Change Keyboard Layout `-- Generic 105-key (Intl) PC `-- English (US) `-- The default for the keyboard layout `-- No compose key `-- No X server termination hotkey","title":"Raspberry Pi Configurations"},{"location":"tutorials/configuration/#wi-fi-configuration","text":"To configure Wi-Fi manually, there are two methods: Use the raspi-config command and the GUI interface. Edit /etc/wpa_supplicant/wpa_supplicant.conf . A working example looks like below: $ cat wpa_supplicant.conf ctrl_interface=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=TW network={ ssid=\"MyAP\" psk=\"B3rryN3t\" key_mgmt=WPA-PSK }","title":"Wi-Fi Configuration"},{"location":"tutorials/docker/","text":"How to run BerryNet in docker This page describe how to run BerryNet inside the Docker. The host os is currently on Debian or Ubuntu. Create a berrynet-buster image. docker build -t berrynet-buster ./docker Create a privileged Ubuntu docker instance. Because BerryNet contains several services which needs systemd to load and run. Thus we start with /sbin/init. Also we need to allow to use USB camera inside. docker run -d --privileged -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /dev/bus/usb:/dev/bus/usb --device /dev/video0:/dev/video0 berrynet-buster /sbin/init Login into the docker docker exec -i -t <instance ID> /bin/bash Use BerryNet inside the docker You need to figure out the IP address of the docker and then use it. Change the IP address inside the config file from localhost to 172.x.x.x Now we can use the browser to access the dashboard outside of the docker through 172.x.x.x IP address.","title":"BerryNet on Docker"},{"location":"tutorials/docker/#how-to-run-berrynet-in-docker","text":"This page describe how to run BerryNet inside the Docker. The host os is currently on Debian or Ubuntu.","title":"How to run BerryNet in docker"},{"location":"tutorials/docker/#create-a-berrynet-buster-image","text":"docker build -t berrynet-buster ./docker","title":"Create a berrynet-buster image."},{"location":"tutorials/docker/#create-a-privileged-ubuntu-docker-instance","text":"Because BerryNet contains several services which needs systemd to load and run. Thus we start with /sbin/init. Also we need to allow to use USB camera inside. docker run -d --privileged -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /dev/bus/usb:/dev/bus/usb --device /dev/video0:/dev/video0 berrynet-buster /sbin/init","title":"Create a privileged Ubuntu docker instance."},{"location":"tutorials/docker/#login-into-the-docker","text":"docker exec -i -t <instance ID> /bin/bash","title":"Login into the docker"},{"location":"tutorials/docker/#use-berrynet-inside-the-docker","text":"You need to figure out the IP address of the docker and then use it. Change the IP address inside the config file from localhost to 172.x.x.x Now we can use the browser to access the dashboard outside of the docker through 172.x.x.x IP address.","title":"Use BerryNet inside the docker"},{"location":"tutorials/hello/","text":"Goal This tutorial will share how to create a simple surveillance system in your home. Hardwares In this tutorial, we will use these HWs: Raspberry Pi 3 or 4 RPi camera v2 Setup After followed the steps in Installation , your RPi should be ready to run BerryNet. Run BerryNet as Surveillance Application Launch BerryNet BerryNet starts an example detection application by default: All the services' status should be \"RUNNING\". Launch Dashboard Client We will see the detection result on Freeboard by following the steps in README. Launch Camera Client Camera client is started automatically after booted, so RPi camera or USB camera will keep capturing frame in FPS 6 in background. The dashboard will keep updating the inference image and text results on it: For Raspberry Pi 3 Please change the camera client's FPS from 6 to 2 in the default configuration , and reboot the system. Next To stop the system, you can execute $ sudo supervisorcal stop all If you want to Do some data analysis, you can also enable [[Data Collector]] to collect the inference results. Know more about Freeboard, please refer to the open dashboard on your computer section in the [[Dashboard]] document. Send testing images manually instead of using camera stream, please refer to the Camera Client Configuration in README. Know more about camera client, please refer to the [[Cameras]] document for more details. Hoping that this starting point can help you create your own interesting project!","title":"Hello World"},{"location":"tutorials/hello/#goal","text":"This tutorial will share how to create a simple surveillance system in your home.","title":"Goal"},{"location":"tutorials/hello/#hardwares","text":"In this tutorial, we will use these HWs: Raspberry Pi 3 or 4 RPi camera v2","title":"Hardwares"},{"location":"tutorials/hello/#setup","text":"After followed the steps in Installation , your RPi should be ready to run BerryNet.","title":"Setup"},{"location":"tutorials/hello/#run-berrynet-as-surveillance-application","text":"","title":"Run BerryNet as Surveillance Application"},{"location":"tutorials/hello/#launch-berrynet","text":"BerryNet starts an example detection application by default: All the services' status should be \"RUNNING\".","title":"Launch BerryNet"},{"location":"tutorials/hello/#launch-dashboard-client","text":"We will see the detection result on Freeboard by following the steps in README.","title":"Launch Dashboard Client"},{"location":"tutorials/hello/#launch-camera-client","text":"Camera client is started automatically after booted, so RPi camera or USB camera will keep capturing frame in FPS 6 in background. The dashboard will keep updating the inference image and text results on it:","title":"Launch Camera Client"},{"location":"tutorials/hello/#for-raspberry-pi-3","text":"Please change the camera client's FPS from 6 to 2 in the default configuration , and reboot the system.","title":"For Raspberry Pi 3"},{"location":"tutorials/hello/#next","text":"To stop the system, you can execute $ sudo supervisorcal stop all If you want to Do some data analysis, you can also enable [[Data Collector]] to collect the inference results. Know more about Freeboard, please refer to the open dashboard on your computer section in the [[Dashboard]] document. Send testing images manually instead of using camera stream, please refer to the Camera Client Configuration in README. Know more about camera client, please refer to the [[Cameras]] document for more details. Hoping that this starting point can help you create your own interesting project!","title":"Next"},{"location":"tutorials/installation/","text":"There are two ways to install BerryNet: Install from pre-built image Install from source If the reference command listed below starts with host : it means you should do it on the host machine (a Linux PC is preferred). rpi : it means you should do it on Raspberry Pi. Install From Pre-Built Image Download the latest image . To check the integrity of the downloaded image, you can check the md5sum of the image and compare it with the checksum file . Assuming that the image is saved to /home/DT42/Downloads/ directory. host $ md5sum /home/DT42/Downloads/2019-07-15-raspbian-buster-berrynet.zip 8bda8053a69995814a0ca6fa60b1d49e 2019-07-15-raspbian-buster-berrynet.zip You can get older releases from prior release notes Write image to SD card Assuming that the inserted SD card is represented as /dev/sdb on system. (If you see other similar device notes like /dev/sdb1 or /dev/sdb2 , you can ignore them without causing any problem.) host $ unzip -p 2019-07-15-raspbian-buster-berrynet.zip | sudo dd of=/dev/sdb bs=4M conv=fsync status=progress Your SD card size has to be >= 8GB . Boot RPi with the SD card, and login. Connect keyboard, mouse, and monitor to RPi. Insert SD card to RPi. Connect camera to RPi. Connect power adapter to RPi. Login, the default user name and password are username: pi password: raspberry (Optional) Setup Wifi, if you plan to use Wifi to send out inference results or if you plan check the dashboard from another machine instead of connecting the monitor directly to Raspberry Pi. Click the network icon at top-right of desktop, and select your country code and Wifi AP. (Optional) Resize filesystem size manually, if your SD card size is > 8GB and want to use its all storage Launch raspi-config rpi $ sudo raspi-config and choose \"Advanced Options\" -> \"Expend Filesystem\". (Optional) Update to the latest BerryNet The default BerryNet in the image is verified. If you are interested in the latest features, you can update BerryNet by executing the commands: rpi $ sudo apt update rpi $ sudo apt install -y berrynet Reboot RPi and login again. BerryNet default detection application is running in background after booting. Install From Source Before BerryNet installation, please creating a system SD card by following the RPi official installation guide . Here are the steps to install BerryNet on RPi: Get BerryNet source codes rpi $ git clone https://github.com/DT42/BerryNet.git rpi $ cd BerryNet Execute installation script rpi $ ./configure The installation will take a while. If the installation is completed successfully, next step is to configure your BerryNet. Meet Troubles If the installation fails, you can Check is it an known issue Create an issue report and attach <berrynet-dir>/berrynet-install.log Share the problem with the community and we will try our best to check it. Release History All the Raspbian images is available on https://downloads.raspberrypi.org/ BerryNet releases base v3.7.0: 2019-07-10 buster v3.5.1: 2019-04-08 stretch In The Future We are packaging BerryNet and its dependencies to be Debian packages. The long-term goal is to make all the packages followed Debian policy can be available in Debian by default.","title":"Installation"},{"location":"tutorials/installation/#install-from-pre-built-image","text":"Download the latest image . To check the integrity of the downloaded image, you can check the md5sum of the image and compare it with the checksum file . Assuming that the image is saved to /home/DT42/Downloads/ directory. host $ md5sum /home/DT42/Downloads/2019-07-15-raspbian-buster-berrynet.zip 8bda8053a69995814a0ca6fa60b1d49e 2019-07-15-raspbian-buster-berrynet.zip You can get older releases from prior release notes Write image to SD card Assuming that the inserted SD card is represented as /dev/sdb on system. (If you see other similar device notes like /dev/sdb1 or /dev/sdb2 , you can ignore them without causing any problem.) host $ unzip -p 2019-07-15-raspbian-buster-berrynet.zip | sudo dd of=/dev/sdb bs=4M conv=fsync status=progress Your SD card size has to be >= 8GB . Boot RPi with the SD card, and login. Connect keyboard, mouse, and monitor to RPi. Insert SD card to RPi. Connect camera to RPi. Connect power adapter to RPi. Login, the default user name and password are username: pi password: raspberry (Optional) Setup Wifi, if you plan to use Wifi to send out inference results or if you plan check the dashboard from another machine instead of connecting the monitor directly to Raspberry Pi. Click the network icon at top-right of desktop, and select your country code and Wifi AP. (Optional) Resize filesystem size manually, if your SD card size is > 8GB and want to use its all storage Launch raspi-config rpi $ sudo raspi-config and choose \"Advanced Options\" -> \"Expend Filesystem\". (Optional) Update to the latest BerryNet The default BerryNet in the image is verified. If you are interested in the latest features, you can update BerryNet by executing the commands: rpi $ sudo apt update rpi $ sudo apt install -y berrynet Reboot RPi and login again. BerryNet default detection application is running in background after booting.","title":"Install From Pre-Built Image"},{"location":"tutorials/installation/#install-from-source","text":"Before BerryNet installation, please creating a system SD card by following the RPi official installation guide . Here are the steps to install BerryNet on RPi: Get BerryNet source codes rpi $ git clone https://github.com/DT42/BerryNet.git rpi $ cd BerryNet Execute installation script rpi $ ./configure The installation will take a while. If the installation is completed successfully, next step is to configure your BerryNet.","title":"Install From Source"},{"location":"tutorials/installation/#meet-troubles","text":"If the installation fails, you can Check is it an known issue Create an issue report and attach <berrynet-dir>/berrynet-install.log Share the problem with the community and we will try our best to check it.","title":"Meet Troubles"},{"location":"tutorials/installation/#release-history","text":"All the Raspbian images is available on https://downloads.raspberrypi.org/ BerryNet releases base v3.7.0: 2019-07-10 buster v3.5.1: 2019-04-08 stretch","title":"Release History"},{"location":"tutorials/installation/#in-the-future","text":"We are packaging BerryNet and its dependencies to be Debian packages. The long-term goal is to make all the packages followed Debian policy can be available in Debian by default.","title":"In The Future"}]}